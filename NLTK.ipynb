{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYJOOdk0OLDPMaLgxdkqgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guidevit/notebooks/blob/main/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with NLTK"
      ],
      "metadata": {
        "id": "plb61z0ZHB6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, ensure you have NLTK installed. If not, you can install it via pip:"
      ],
      "metadata": {
        "id": "gU6bUn-2HFGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mUu7gnGGAwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b91dfa-8619-41b0-ac71-4a61880b0b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installation, you can start by importing NLTK and downloading the necessary datasets and models:"
      ],
      "metadata": {
        "id": "AAJQBTO_HLAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvvoD8wOGaOp",
        "outputId": "8954cac6-d2a5-49b5-ed1d-5c27b9b4a7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This downloads the most popular resources, including corpora and models for tokenization, parsing, and tagging, which can be quite handy for many NLP tasks."
      ],
      "metadata": {
        "id": "lT62D4OzHRYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Concepts and Operations in NLTK"
      ],
      "metadata": {
        "id": "DXIPSFVKHTDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenization\n",
        "Tokenization is the process of breaking down a string into tokens, which can be words or sentences. This is often the first step in text processing."
      ],
      "metadata": {
        "id": "rD0vGBdQHU9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello there! how are you today? This is an example sentence.\"\n",
        "\n",
        "print(word_tokenize(text)) # Word tokenization\n",
        "print(sent_tokenize(text)) # Sentence tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bvfzyI1GcUh",
        "outputId": "8f44e1eb-928e-4c44-91a1-d08596e5d262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'there', '!', 'how', 'are', 'you', 'today', '?', 'This', 'is', 'an', 'example', 'sentence', '.']\n",
            "['Hello there!', 'how are you today?', 'This is an example sentence.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Stopwords Removal\n",
        "Stopwords are common words like \"is\", \"an\", \"the\", etc., that are often removed during preprocessing to reduce noise."
      ],
      "metadata": {
        "id": "V9hKwk_gHZ1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWKNd0dSG6so",
        "outputId": "2b5b35a9-f17b-42b3-d700-750a66918a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'today', '?', 'example', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Stemming and Lemmatization\n",
        "Stemming is the process of reducing words to their word stem (base form). Lemmatization, a more sophisticated approach, reduces words to their base or dictionary form."
      ],
      "metadata": {
        "id": "Lys6hPPvH8HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = \"constitution\"\n",
        "print(stemmer.stem(word))\n",
        "print(lemmatizer.lemmatize(word, pos='v')) # 'v' denotes verb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwtcuat7H0zj",
        "outputId": "14467693-8159-4b46-c245-d4202df720b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constitut\n",
            "constitution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple NLP Model\n",
        "\n",
        "Let's say you want to build a basic sentiment analysis model using NLTK. You would typically follow these steps:\n",
        "\n",
        "1. Preprocess your data: Tokenize text, remove stopwords, and maybe use stemming or lemmatization.\n",
        "2. Feature Extraction: Convert text to a numerical format, using techniques like Bag-of-Words or TF-IDF.\n",
        "3. Model Training: Use a machine learning algorithm to train a model on your features. NLTK integrates well with Scikit-learn for this purpose.\n",
        "4. Evaluation: Test your model on unseen data and evaluate its performance using metrics like accuracy, precision, and recall."
      ],
      "metadata": {
        "id": "qAZwt5_QI6_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a simple sentiment analysis model together step by step. We'll go through each stage of the process, from preprocessing the data to evaluating the model. We'll use a small sample dataset for demonstration purposes."
      ],
      "metadata": {
        "id": "8E5lJVwsJPkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Preprocess the Data\n",
        "\n",
        "We start by importing the necessary libraries and preparing our text data. We'll perform tokenization, remove stopwords, and apply lemmatization.\n",
        "\n",
        "First, make sure you have Scikit-learn installed. If not, you can install them using pip:"
      ],
      "metadata": {
        "id": "0MrSlkb8JS3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3OmWO4-Ia8m",
        "outputId": "013c4a24-5f89-4325-903d-9df5e41d7a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write some code to preprocess our data:"
      ],
      "metadata": {
        "id": "9_IlJ88CJcj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text data (Replace this with your dataset)\n",
        "text_samples = [\"This is a great movie\",\n",
        "                \"I did not like the film\",\n",
        "                \"Amazing script, but the acting was bad\",\n",
        "                \"Not my cup of tea\",\n",
        "                \"Exceptional cinematography\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Sa_U3VJfJh",
        "outputId": "f8baa457-d50b-43d6-f7bc-ac4d99530d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  # Tokenization\n",
        "  words = word_tokenize(text)\n",
        "\n",
        "  # Removing stopwords and lemmatizing\n",
        "  filtered_words = [lemmatizer.lemmatize(word.lower())\n",
        "                    for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "  return \" \".join(filtered_words)\n",
        "\n",
        "# Preprocess all text samples\n",
        "preprocessed_texts = [preprocess_text(text) for text in text_samples]\n",
        "print(preprocessed_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv9k_RGoK1Bd",
        "outputId": "8b0488c5-50c3-4c7f-b206-cf45bdf7c594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great movie', 'like film', 'amazing script acting bad', 'cup tea', 'exceptional cinematography']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Feature Extraction\n",
        "Next, we'll convert our preprocessed text into a numerical format using the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer."
      ],
      "metadata": {
        "id": "ZGiOKKx5JfhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the preprocessed texts\n",
        "X = vectorizer.fit_transform(preprocessed_texts)\n",
        "\n",
        "print(X.shape)  # Check the shape of the resulting feature matrix\n"
      ],
      "metadata": {
        "id": "T8WpPYslJgw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6f42ae-4b8a-4743-850c-92dd8e724754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Model Training\n",
        "For this example, we'll use a simple Naive Bayes classifier, which is commonly used for text classification tasks, including sentiment analysis."
      ],
      "metadata": {
        "id": "1BPT62ZqJg42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample labels for our text (1 for positive, 0 for negative)\n",
        "# Note: Make sure to align these with your actual data\n",
        "y = [1, 0, 0, 0, 1]\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "VEJsbOMWJjvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d245707c-cde5-43ff-9144-89b6131b3164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluation\n",
        "In the code above, we've already included a basic evaluation using accuracy. For more detailed insights, you can also compute other metrics such as precision, recall, and F1-score:"
      ],
      "metadata": {
        "id": "YeTxwwNuJj4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Detailed performance analysis\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "EiHhQy97JqPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7014e945-d7ea-491c-adf1-7bdae5c02c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67         1\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This walkthrough provides a basic framework for building a sentiment analysis model using NLTK and Scikit-learn. Depending on your specific needs, you may want to experiment with different preprocessing techniques, feature extraction methods, and machine learning models to improve performance."
      ],
      "metadata": {
        "id": "5Py2e0HjJqVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intermetiate Concepts\n",
        "Moving on to more intermediate concepts and applications within NLTK, let’s explore some key areas that can provide deeper insights and improvements to NLP projects. These areas include part-of-speech (POS) tagging, parsing, named entity recognition (NER), and incorporating more sophisticated machine learning models."
      ],
      "metadata": {
        "id": "ZgoG2_JysGWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech Tagging\n",
        "Part-of-speech tagging is the process of assigning a part of speech to each word in a given text, such as noun, verb, adjective, etc. This is useful for building features for text classification, understanding sentence structure, and aiding in entity recognition."
      ],
      "metadata": {
        "id": "6HW_WAUysK_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "text = \"\"\"NLTK is a leading platform for building Python\n",
        "        programs to work with human language data.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G1a_l4TsM_f",
        "outputId": "e7af0f0f-c864-4778-a548-f738145a8fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing\n",
        "Parsing is the process of analyzing the grammatical structure of a sentence. This can be useful for extracting relationships between words and understanding the context better."
      ],
      "metadata": {
        "id": "ATE7w3YGtIlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# Define a more appropriate grammar\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "  S -> NP VP\n",
        "  VP -> V NP | V NP PP\n",
        "  NP -> DT N\n",
        "  PP -> P NP\n",
        "  DT -> 'the'\n",
        "  N -> 'cat' | 'mat'\n",
        "  V -> 'sat'\n",
        "  P -> 'on'\n",
        "\"\"\")\n",
        "\n",
        "# Prepare the sentence\n",
        "sentence = \"the cat sat on the mat\".split()\n",
        "\n",
        "# Create a parser\n",
        "parser = nltk.ChartParser(grammar)\n",
        "\n",
        "# Parse the sentence and print the parse tree\n",
        "for tree in parser.parse(sentence):\n",
        "    print(tree)\n",
        "    break  # Only print the first tree if there are multiple\n"
      ],
      "metadata": {
        "id": "zIOcWmQBsvkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)\n",
        "Named Entity Recognition is a process where you identify important entities within the text, such as the names of people, places, organizations, dates, etc. It’s crucial for information extraction, content classification, and more."
      ],
      "metadata": {
        "id": "YkLr8lKYuXIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sentence = \"Apple Inc. announced the new iPhone in San Francisco\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-snx-wTtnUX",
        "outputId": "0fa30055-b6bc-46b0-e688-81ae64545650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  (ORGANIZATION Inc./NNP)\n",
            "  announced/VBD\n",
            "  the/DT\n",
            "  new/JJ\n",
            "  iPhone/NN\n",
            "  in/IN\n",
            "  (GPE San/NNP)\n",
            "  Francisco/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Machine Learning Models\n",
        "While NLTK provides basic tools for text processing and classification, integrating it with more advanced machine learning libraries like Scikit-learn or TensorFlow can significantly enhance your NLP projects.\n",
        "\n",
        "For instance, you can use NLTK for preprocessing and feature extraction, and then train a more sophisticated model such as a Support Vector Machine (SVM) or a neural network for tasks like sentiment analysis or text classification."
      ],
      "metadata": {
        "id": "s-ODRi-qvB8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming X and y are your features and labels respectively\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_texts, y, test_size=0.25)\n",
        "\n",
        "# Using a TF-IDF vectorizer and an SVM classifier\n",
        "model = make_pipeline(TfidfVectorizer(), SVC(kernel='linear'))\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting and evaluating\n",
        "predicted = model.predict(X_test)\n",
        "print(accuracy_score(y_test, predicted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUrFnsK1uzyU",
        "outputId": "04cefa4c-1103-4e67-934a-391b7612b073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These intermediate concepts and practices in NLTK pave the way for tackling complex NLP challenges. Experimenting with different techniques, exploring NLTK's vast array of features, and integrating with external machine learning libraries can help you build robust and efficient NLP applications."
      ],
      "metadata": {
        "id": "ob0BpWy1vb1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Concepts\n",
        "Diving into advanced concepts and use cases of NLTK involves exploring deeper functionalities that allow for sophisticated text analysis and natural language understanding. These advanced areas include working with large text corpora, advanced machine learning techniques, deep learning integration, and handling multilingual text. Here's how you can leverage NLTK for these advanced NLP tasks:"
      ],
      "metadata": {
        "id": "hELnWpSEvkmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Large Text Corpora\n",
        "NLTK provides access to a wide range of text corpora and lexical resources. For advanced use, you might need to work with large corpora or even combine multiple corpora to build robust language models."
      ],
      "metadata": {
        "id": "pvP7aj3gvqfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown, reuters\n",
        "nltk.download('brown')\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Example of accessing a large corpus\n",
        "brown_words = brown.words()\n",
        "reuters_words = reuters.words()\n",
        "print(f\"Number of words in Brown Corpus: {len(brown_words)}\")\n",
        "print(f\"Number of words in Reuters Corpus: {len(reuters_words)}\")\n",
        "\n",
        "# You can perform complex analyses, train language models, or extract features from these large datasets.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohosLpfuvPf3",
        "outputId": "3f740236-9945-49f0-d1c4-6dd261228138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in Brown Corpus: 1161192\n",
            "Number of words in Reuters Corpus: 1720901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Machine Learning Techniques\n",
        "Beyond basic classification tasks, you can use NLTK to preprocess data for more advanced machine learning tasks like topic modeling, sentiment analysis with aspect mining, or complex classification schemes involving hierarchical or multi-label classification.\n",
        "\n",
        "Integrating NLTK with libraries like gensim for topic modeling or scikit-learn for advanced classifiers (e.g., ensemble methods) can unlock deeper insights into your text data."
      ],
      "metadata": {
        "id": "Hw9NvCbkwBGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Advanced ML pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Assuming X_train and y_train are prepared datasets\n",
        "pipeline.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "ktfi61CQv6La",
        "outputId": "5638a7d2-5be2-48a1-abfc-d743e6cdecbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()), ('clf', RandomForestClassifier())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;clf&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;clf&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Integration\n",
        "For tasks requiring an understanding of context or the nuances of language (like question answering, machine translation, or sentiment analysis), integrating NLTK with deep learning frameworks like TensorFlow or PyTorch is crucial. You can use NLTK for data preprocessing and then feed the processed data into deep learning models."
      ],
      "metadata": {
        "id": "qAhkDjQ1wlqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example pseudocode for integrating NLTK with a deep learning framework\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "# Preprocess your text data using NLTK, then convert it to sequences or embeddings compatible with Keras/TensorFlow.\n",
        "\n",
        "# Build a deep learning model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=1000, output_dim=64))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "jPy-8hnnwQyy",
        "outputId": "8f91becd-1987-4b6c-cea7-2ce3f77a7ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'str'>, <class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'int'>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-dd9050409f06>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Compile and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1774\u001b[0m     \u001b[0munsplitable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_arrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_can_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munsplitable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1777\u001b[0m             \u001b[0;34m\"`validation_split` is only supported for Tensors or NumPy \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m             \u001b[0;34m\"arrays, found following types in the input: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsplitable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'str'>, <class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'int'>]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Multilingual Text\n",
        "NLTK supports various languages for basic tasks like tokenization and part-of-speech tagging. For advanced multilingual NLP, you can preprocess text in different languages using NLTK and then apply cross-lingual or language-specific models for analysis or translation."
      ],
      "metadata": {
        "id": "erBW_I26xFYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization in different languages\n",
        "spanish_text = \"Esto es un texto en español.\"\n",
        "german_text = \"Dies ist ein Text auf Deutsch.\"\n",
        "\n",
        "print(word_tokenize(spanish_text, language='spanish'))\n",
        "print(word_tokenize(german_text, language='german'))\n",
        "\n",
        "# For advanced multilingual tasks, you might integrate NLTK processed data with models trained specifically for those languages.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so61_4ofwymF",
        "outputId": "f0446269-13e4-4152-ebdc-a54e89929da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Esto', 'es', 'un', 'texto', 'en', 'español', '.']\n",
            "['Dies', 'ist', 'ein', 'Text', 'auf', 'Deutsch', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis with Aspect Mining\n",
        "Going beyond simple positive/negative sentiment analysis, aspect-based sentiment analysis involves identifying sentiments towards specific aspects of a product or service. NLTK can be used to preprocess and identify potential aspects by noun phrase extraction or dependency parsing, which can then be analyzed for sentiment."
      ],
      "metadata": {
        "id": "t0-60E1cxRYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudocode for aspect-based sentiment analysis\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk import pos_tag, word_tokenize, ne_chunk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "text = \"The battery life of this phone is too short, but its camera quality is outstanding.\"\n",
        "\n",
        "# Extract aspects (e.g., battery life, camera quality)\n",
        "# Analyze sentiment towards each aspect\n",
        "sentiments = sia.polarity_scores(text)\n",
        "print(sentiments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV6bc4sxxMuj",
        "outputId": "da568ddc-94a4-4ab1-e53d-b7c756574a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.718, 'pos': 0.282, 'compound': 0.7579}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These advanced applications and techniques showcase the flexibility and power of NLTK when combined with other Python libraries and frameworks. As you venture into these complex tasks, remember that the key to successful NLP projects lies in thorough preprocessing, innovative feature engineering, and the careful selection and tuning of machine learning or deep learning models"
      ],
      "metadata": {
        "id": "cEYNBUSlxmyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical real world cases\n",
        "For more advanced and practical real-world NLP cases using NLTK, let’s dive into specific scenarios where NLP can provide significant insights or automation capabilities. These scenarios will cover sentiment analysis in social media, automating customer support with chatbots, language detection and translation, and text summarization."
      ],
      "metadata": {
        "id": "ypiFTcpWxvVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Sentiment Analysis in Social Media\n",
        "Businesses often monitor social media to gauge public sentiment about their brand, products, or services. This involves collecting social media posts and analyzing them for positive, negative, or neutral sentiments.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZdA2aSmPx01r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_social_media_post(post):\n",
        "    score = sia.polarity_scores(post)\n",
        "    return score\n",
        "\n",
        "# Example social media post\n",
        "post = \"I absolutely love the new product! It has changed my daily routine for the better.\"\n",
        "analysis_result = analyze_social_media_post(post)\n",
        "print(analysis_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTyIkESVxZi3",
        "outputId": "10a5d0b0-0c23-40bb-f5ff-47b4a65f3fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compound': 0.8264}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Automating Customer Support with Chatbots\n",
        "NLTK can be used to build basic chatbots that automate responses to common customer inquiries, improving efficiency and customer satisfaction."
      ],
      "metadata": {
        "id": "AzH3i8h6yGf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chat.util import Chat, reflections\n",
        "\n",
        "pairs = [\n",
        "    [r\"hi|hello|hey\", [\"Hello, how can I help you today?\"]],\n",
        "    [r\"(.*) your name?\", [\"I'm NLTKBot, your virtual assistant.\"]],\n",
        "    [r\"(.*) created you?\", [\"I was created by an NLTK NLP Engineer.\"]],\n",
        "    [r\"how can I (.*) help?\", [\"I can assist you with your questions or direct you to the right resources.\"]],\n",
        "    [r\"quit\", [\"Bye, have a great day!\"]]\n",
        "]\n",
        "\n",
        "chatbot = Chat(pairs, reflections)\n",
        "chatbot.converse()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWZmT326yBHR",
        "outputId": "816c7599-1978-402d-c2c3-f2ecf4d2b2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">Hello\n",
            "Hello, how can I help you today?\n",
            ">your name?\n",
            "None\n",
            ">I need herlp with my purchase\n",
            "None\n",
            ">hi\n",
            "Hello, how can I help you today?\n",
            ">What is your name?\n",
            "I'm NLTKBot, your virtual assistant.\n",
            ">who created you?\n",
            "I was created by an NLTK NLP Engineer.\n",
            ">how can i fucking help?\n",
            "I can assist you with your questions or direct you to the right resources.\n",
            ">QUIT\n",
            "Bye, have a great day!\n",
            ">quit\n",
            "Bye, have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Language Detection and Translation\n",
        "For global applications, detecting the language of the input text and translating text can be crucial. While NLTK provides basic tools for language processing, integration with libraries like polyglot or services like Google Translate API might be necessary for translation tasks."
      ],
      "metadata": {
        "id": "ZQL5OOUvyoTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a conceptual demonstration. For actual implementation, use 'polyglot' or Google Translate API.\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def detect_language(text):\n",
        "    languages_ratios = {}\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [word.lower() for word in tokens]\n",
        "\n",
        "    for language in stopwords.fileids():\n",
        "        stopwords_set = set(stopwords.words(language))\n",
        "        words_set = set(words)\n",
        "        common_elements = words_set.intersection(stopwords_set)\n",
        "\n",
        "        languages_ratios[language] = len(common_elements)  # Number of common elements with this language's stopwords\n",
        "\n",
        "    most_rated_language = max(languages_ratios, key=languages_ratios.get)\n",
        "    return most_rated_language\n",
        "\n",
        "# Example text\n",
        "text = \"Este é uma frase em uma lingua.\"\n",
        "print(detect_language(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVGkN6-JyJlU",
        "outputId": "71435ad8-2155-4b41-af07-cf382e9a03c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "portuguese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Summarization\n",
        "Text summarization can be helpful in digesting large volumes of information quickly, such as summarizing news articles, research papers, or customer reviews."
      ],
      "metadata": {
        "id": "cLGX3lrry3wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def summarize_text(text, n=5):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Frequency distribution of words\n",
        "    freq_dist = FreqDist(w.lower() for w in words if w not in stop_words and w.isalpha())\n",
        "    ranking_sentences = {}\n",
        "\n",
        "    for i, sent in enumerate(sent_tokenize(text)):\n",
        "        for w in word_tokenize(sent.lower()):\n",
        "            if w in freq_dist:\n",
        "                if i not in ranking_sentences:\n",
        "                    ranking_sentences[i] = freq_dist[w]\n",
        "                else:\n",
        "                    ranking_sentences[i] += freq_dist[w]\n",
        "\n",
        "    # Identify top n sentences\n",
        "    top_sentences = sorted(ranking_sentences, key=ranking_sentences.get, reverse=True)[:n]\n",
        "\n",
        "    return [sent_tokenize(text)[j] for j in sorted(top_sentences)]\n",
        "\n",
        "# Example text (use a longer text for better results)\n",
        "text = \"\"\"\n",
        "GPT4All is a framework focused on enabling powerful LLMs to run locally on consumer-grade CPUs in laptops, tablets, smartphones, or single-board computers. These LLMs can do everything ChatGPT and GPT Assistants can, including:\n",
        "\n",
        "Answer questions on just about any topic imaginable\n",
        "Understand complex documents of personal or professional importance and provide useful answers related to their contents\n",
        "Help compose emails, documents, stories, poems, or songs\n",
        "Generate code — even entire applications — using popular programming languages and frameworks\n",
        "GPT4All provides an ecosystem of building blocks to help you train and deploy customized, locally running, LLM-powered chatbots. These building blocks include:\n",
        "\n",
        "GPT4All open-source models: The GPT4All LLMs are fine-tuned for assistant-style, multi-turn conversations that can run on commodity CPUs without any need for expensive graphics processing units (GPUs) or tensor processing units.\n",
        "GPT4All desktop chatbot: The GPT4All desktop assistant-style chatbot can run on commodity processors and popular operating systems like Windows, macOS, and Linux.\n",
        "GPT4All software components: GPT4All releases chatbot building blocks that third-party applications can use. They include scripts to train and prepare custom models that run on commodity CPUs.\n",
        "GPT4All dataset: The GPT4All training dataset can be used to train or fine-tune GPT4All models and other chatbot models.\n",
        "GPT4All is backed by Nomic.ai's team of Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Adam Treat, and Andriy Mulyar. They have explained the GPT4All ecosystem and its evolution in three technical reports:\n",
        "\n",
        "GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo\n",
        "GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot\n",
        "GPT4All: An ecosystem of open-source assistants that run on local hardware\n",
        "\"\"\"\n",
        "summary = summarize_text(text)\n",
        "print(\" \".join(summary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-rb7SItyuKB",
        "outputId": "5b2488df-b0dc-4742-be0f-b113f8612cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These LLMs can do everything ChatGPT and GPT Assistants can, including:\n",
            "\n",
            "Answer questions on just about any topic imaginable\n",
            "Understand complex documents of personal or professional importance and provide useful answers related to their contents\n",
            "Help compose emails, documents, stories, poems, or songs\n",
            "Generate code — even entire applications — using popular programming languages and frameworks\n",
            "GPT4All provides an ecosystem of building blocks to help you train and deploy customized, locally running, LLM-powered chatbots. These building blocks include:\n",
            "\n",
            "GPT4All open-source models: The GPT4All LLMs are fine-tuned for assistant-style, multi-turn conversations that can run on commodity CPUs without any need for expensive graphics processing units (GPUs) or tensor processing units. GPT4All desktop chatbot: The GPT4All desktop assistant-style chatbot can run on commodity processors and popular operating systems like Windows, macOS, and Linux. GPT4All dataset: The GPT4All training dataset can be used to train or fine-tune GPT4All models and other chatbot models. They have explained the GPT4All ecosystem and its evolution in three technical reports:\n",
            "\n",
            "GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo\n",
            "GPT4All-J: An Apache-2 Licensed Assistant-Style Chatbot\n",
            "GPT4All: An ecosystem of open-source assistants that run on local hardware\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Exercise\n",
        "Preprocessing a large amount of chat data for fine-tuning an open-source language model involves several steps to ensure the text is clean, structured, and ready for model consumption. This process typically includes tokenization, removal of unnecessary elements (like stop words or irrelevant punctuation), possibly normalization (like lowercasing), and then structuring the data in a way that's suitable for the language model training process.\n",
        "\n",
        "Let's break down these steps and execute them with a practical approach:\n",
        "\n"
      ],
      "metadata": {
        "id": "NEmqHW9D0BlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Your Dataset\n",
        "First, you need to load your chat data into Python. Assuming your chat data is in a JSON or CSV file, you can use the appropriate Python libraries (json or pandas) to load the data. For demonstration purposes, let's assume it's in a CSV file:"
      ],
      "metadata": {
        "id": "825QO7-H0NLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F16UqEL50OtL",
        "outputId": "469cad4f-52e8-4c10-90a8-8d9ad8905cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Installing collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import unidecode\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhFFPFRay69a",
        "outputId": "a1a5074c-519f-45d3-da8c-fdd3ab0ebc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define Preprocessing Functions\n",
        "We'll define a function to clean the text (remove punctuation, numbers, and special characters), remove accents, tokenize the text, and remove stopwords."
      ],
      "metadata": {
        "id": "Vy1TAxjz0W7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, language='portuguese', remove_stopwords=True):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove accents\n",
        "    text = unidecode.unidecode(text)\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^a-zà-ú\\s]', '', text)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text, language=language)\n",
        "\n",
        "    # Optionally remove stopwords\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words(language))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "X2NqmnLE0Uat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset\n",
        "texts = [\n",
        "    \"Este é um texto em Português do Brasil.\",\n",
        "    \"Aqui, vamos demonstrar como pré-processar textos grandes.\",\n",
        "    \"Natural Language Processing com NLTK.\"\n",
        "]\n",
        "\n",
        "# Preprocess all texts\n",
        "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# Output the processed tokens\n",
        "for tokens in preprocessed_texts:\n",
        "    print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2XCm-BN0d5v",
        "outputId": "dd353c8d-5bbd-4096-f097-f6de26c7646c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['texto', 'portugues', 'brasil']\n",
            "['aqui', 'vamos', 'demonstrar', 'preprocessar', 'textos', 'grandes']\n",
            "['natural', 'language', 'processing', 'nltk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bF8ryfz10lGA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}