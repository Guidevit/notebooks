{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13E5oJZdw2tZO-S8k8AnTivg8M8TY6mh-",
      "authorship_tag": "ABX9TyM47LinujVLylsG4zAFhOAi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guidevit/notebooks/blob/main/Pipeline_RAG_com_Evaluation_Juridico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQHhT3osJPjV"
      },
      "outputs": [],
      "source": [
        "!pip -q install python-dotenv pinecone-client llama-index pymupdf llmsherpa openai llama_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SET ENVIRONMENT\n",
        "\n",
        "We create a file for our environment variables."
      ],
      "metadata": {
        "id": "gaHFOkJxKpx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pinecone\n",
        "pinecone.init(api_key=\"\", environment=\"\")\n",
        "openai.api_key = \"\"\n"
      ],
      "metadata": {
        "id": "kvq5RhhAJQXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SETUP\n",
        "\n",
        "We build an empty Pinecone Index, and define the necessary LlamaIndex wrappers/abstractions so that we can start loading data into Pinecone."
      ],
      "metadata": {
        "id": "1Ytu9DlwPIjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"PINECONE_API_KEY\"\n",
        "environment = \"PINECONE_ENVIRONMENT\"\n",
        "pinecone.init(api_key=api_key, environment=environment)"
      ],
      "metadata": {
        "id": "eclD8kX0PeUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build an Ingestion Pipeline from Scratch\n",
        "\n",
        "We show how to build an ingestion pipeline as mentioned in the introduction.\n",
        "\n",
        "Note that steps (2) and (3) can be handled via our NodeParser abstractions, which handle splitting and node creation.\n",
        "\n",
        "For the purposes of this tutorial, we show you how to create these objects manually."
      ],
      "metadata": {
        "id": "0MwXctnBP6v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Load Data"
      ],
      "metadata": {
        "id": "7D2inJc7QHvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "\n",
        "file_path = \"\"\n",
        "doc = fitz.open(file_path)"
      ],
      "metadata": {
        "id": "uMzrukyvP6GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Use a Text Splitter do Split Documents\n",
        "\n",
        "Here we import our SentenceSplitter to split document texts into smaller chunks, while preserving paragraphs/sentences as much as possible."
      ],
      "metadata": {
        "id": "PIhIOxgdQcdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.text_splitter import SentenceSplitter\n"
      ],
      "metadata": {
        "id": "4pToZ2CYQkcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SentenceSplitter(\n",
        "    chunk_size=1024,\n",
        "    # separator=\" \",\n",
        ")"
      ],
      "metadata": {
        "id": "gtQWAhuoRVDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = []\n",
        "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
        "doc_idxs = []\n",
        "for doc_idx, page in enumerate(doc):\n",
        "    page_text = page.get_text(\"text\")\n",
        "    cur_text_chunks = text_splitter.split_text(page_text)\n",
        "    text_chunks.extend(cur_text_chunks)\n",
        "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
      ],
      "metadata": {
        "id": "EqGydxf9Zstf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Manually Construct Nodes from Text Chunks\n",
        "\n",
        "We convert each chunk into a TextNode object, a low-level data abstraction in LlamaIndex that stores content but also allows defining metadata + relationships with other Nodes.\n",
        "\n",
        "We inject metadata from the document into each node.\n",
        "\n",
        "This essentially replicates logic in our SimpleNodeParser."
      ],
      "metadata": {
        "id": "kDMdFyeDRagJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.schema import TextNode"
      ],
      "metadata": {
        "id": "RvYzpBo2RZ4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = []\n",
        "for idx, text_chunk in enumerate(text_chunks):\n",
        "    node = TextNode(\n",
        "        text=text_chunk,\n",
        "    )\n",
        "    src_doc_idx = doc_idxs[idx]\n",
        "    src_page = doc[src_doc_idx]\n",
        "    nodes.append(node)"
      ],
      "metadata": {
        "id": "xtIDpyh9Rgl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nodes[0].metadata)"
      ],
      "metadata": {
        "id": "wYbn_RUASPcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nodes[2].get_content(metadata_mode=\"all\"))"
      ],
      "metadata": {
        "id": "Qvux3RNHSRFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Extract Metadata from each Node\n",
        "\n",
        "We extract metadata from each Node using our Metadata extractors.\n",
        "\n",
        "This will add more metadata to each Node."
      ],
      "metadata": {
        "id": "pX3M1MWsav2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser.extractors import (\n",
        "    MetadataExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    TitleExtractor,\n",
        ")\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "metadata_extractor = MetadataExtractor(\n",
        "    extractors=[\n",
        "        TitleExtractor(nodes=5, llm=llm),\n",
        "        QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
        "    ],\n",
        "    in_place=False,\n",
        ")"
      ],
      "metadata": {
        "id": "tQXes82OavXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = metadata_extractor.process_nodes(nodes)"
      ],
      "metadata": {
        "id": "xGkRVINpaa9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nodes[12].metadata)"
      ],
      "metadata": {
        "id": "gwvr_l2Xa2rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Generate Embeddings for each Node\n",
        "\n",
        "Generate document embeddings for each Node using our OpenAI embedding model (text-embedding-ada-002).\n",
        "\n",
        "Store these on the embedding property on each Node."
      ],
      "metadata": {
        "id": "sbY7fwEYcliD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import OpenAIEmbedding\n",
        "\n",
        "embed_model = OpenAIEmbedding()"
      ],
      "metadata": {
        "id": "YBNG3WXVck6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for node in nodes:\n",
        "    node_embedding = embed_model.get_text_embedding(\n",
        "        node.get_content(metadata_mode=\"all\")\n",
        "    )\n",
        "    node.embedding = node_embedding"
      ],
      "metadata": {
        "id": "VxyW_gkncMA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "pinecone.init(api_key=\"\", environment=\"us-west1-gcp-free\")\n",
        "pinecone.list_indexes()"
      ],
      "metadata": {
        "id": "sQD_fl35hH62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "\n",
        "index_name = \"...\"\n",
        "\n",
        "pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
      ],
      "metadata": {
        "id": "dpztgoW3gzJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.add(nodes)"
      ],
      "metadata": {
        "id": "hAMvZQugcurQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieve and Query from the Vector Store\n",
        "\n",
        "Now that our ingestion is complete, we can retrieve/query this vector store.\n",
        "\n",
        "NOTE: We can use our high-level VectorStoreIndex abstraction here. See the next section to see how to define retrieval at a lower-level!"
      ],
      "metadata": {
        "id": "IyR-O45jc59e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.storage import StorageContext"
      ],
      "metadata": {
        "id": "JJENmJLqc004"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_vector_store(vector_store)"
      ],
      "metadata": {
        "id": "s2FbhBfZc9Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "pupCJpeYc-WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = \"escreva sobre os requerimentos do autor sobre os danos morais\"\n"
      ],
      "metadata": {
        "id": "pg6eeCGVc_f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(query_str)\n"
      ],
      "metadata": {
        "id": "Cwk-pA_1dHkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "id": "2kCU_7spdJOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Retrieval from Scratch\n",
        "\n",
        "We use Pinecone as the vector database. We load in nodes using our high-level ingestion abstractions (to see how to build this from scratch, see our previous tutorial!).\n",
        "\n",
        "We will show how to do the following:\n",
        "\n",
        "How to generate a query embedding\n",
        "\n",
        "How to query the vector database using different search modes (dense, sparse, hybrid)\n",
        "\n",
        "How to parse results into a set of Nodes\n",
        "\n",
        "How to put this in a custom retriever"
      ],
      "metadata": {
        "id": "uFjGAWZwh58c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_hub"
      ],
      "metadata": {
        "id": "efzeJ7JEi04C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup Pinecone"
      ],
      "metadata": {
        "id": "apH6EHkah-y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "pinecone.init(api_key=\"\", environment=\"us-west1-gcp-free\")"
      ],
      "metadata": {
        "id": "atMaeYSxhkGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"\"\n",
        "\n",
        "pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
      ],
      "metadata": {
        "id": "roiTmip5iSeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WoQgjfhtmFf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Document\n"
      ],
      "metadata": {
        "id": "H62IRJuDimk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"...\""
      ],
      "metadata": {
        "id": "NqNph9mwilyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from llama_hub.file.pymu_pdf.base import PyMuPDFReader"
      ],
      "metadata": {
        "id": "MFjG-O1uipIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyMuPDFReader()\n",
        "documents = loader.load(file_path=file_path)"
      ],
      "metadata": {
        "id": "TU1XR85riuD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load into Vector Store\n",
        "\n",
        "Load in documents into the PineconeVectorStore.\n",
        "\n",
        "NOTE: We use high-level ingestion abstractions here, with VectorStoreIndex.from_documents. We’ll refrain from using VectorStoreIndex for the rest of this tutorial."
      ],
      "metadata": {
        "id": "y2_cPHgsi--B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.storage import StorageContext"
      ],
      "metadata": {
        "id": "dsyWVoW0i1tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context, storage_context=storage_context\n",
        ")"
      ],
      "metadata": {
        "id": "HjM6VZbijEKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Vector Retriever\n",
        "\n",
        "Now we’re ready to define our retriever against this vector store to retrieve a set of nodes.\n",
        "\n",
        "We’ll show the processes step by step and then wrap it into a function."
      ],
      "metadata": {
        "id": "87SIAcFhjI43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = \"Liste os argumentos utilizados pelo autor sobre o dano moral\"\n"
      ],
      "metadata": {
        "id": "p1uxMWaXjFfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Generate a Query Embedding"
      ],
      "metadata": {
        "id": "ySNwJasxjXAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings import OpenAIEmbedding\n",
        "\n",
        "embed_model = OpenAIEmbedding()"
      ],
      "metadata": {
        "id": "j_mi3LcfjSkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding = embed_model.get_query_embedding(query_str)\n"
      ],
      "metadata": {
        "id": "Q7YKODVajakK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Query the Vector Database\n",
        "\n",
        "We show how to query the vector database with different modes: default, sparse, and hybrid.\n",
        "\n",
        "We first construct a VectorStoreQuery and then query the vector db."
      ],
      "metadata": {
        "id": "qfii9otqjgIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct vector store query\n",
        "from llama_index.vector_stores import VectorStoreQuery\n",
        "\n",
        "query_mode = \"default\"\n",
        "# query_mode = \"sparse\"\n",
        "# query_mode = \"hybrid\"\n",
        "\n",
        "vector_store_query = VectorStoreQuery(\n",
        "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
        ")"
      ],
      "metadata": {
        "id": "rDp5mlPvjboz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a VectorStoreQueryResult\n",
        "query_result = vector_store.query(vector_store_query)\n",
        "query_result"
      ],
      "metadata": {
        "id": "2nq0htcVjk_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Parse Result into a set of Nodes\n",
        "\n",
        "The VectorStoreQueryResult returns the set of nodes and similarities. We construct a NodeWithScore object with this."
      ],
      "metadata": {
        "id": "OPZFOKLRjqN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.schema import NodeWithScore\n",
        "from typing import Optional\n",
        "\n",
        "nodes_with_scores = []\n",
        "for index, node in enumerate(query_result.nodes):\n",
        "    score: Optional[float] = None\n",
        "    if query_result.similarities is not None:\n",
        "        score = query_result.similarities[index]\n",
        "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
      ],
      "metadata": {
        "id": "P8WIa5hujmxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.response.notebook_utils import display_source_node\n",
        "\n",
        "for node in nodes_with_scores:\n",
        "    display_source_node(node, source_length=1000)"
      ],
      "metadata": {
        "id": "OmclLx49jt1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Put this into a Retriever\n",
        "\n",
        "Let’s put this into a Retriever subclass that can plug into the rest of LlamaIndex workflows!"
      ],
      "metadata": {
        "id": "6DFtYZalkCch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import QueryBundle\n",
        "from llama_index.retrievers import BaseRetriever\n",
        "from typing import Any, List\n",
        "\n",
        "\n",
        "class PineconeRetriever(BaseRetriever):\n",
        "    \"\"\"Retriever over a pinecone vector store.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_store: PineconeVectorStore,\n",
        "        embed_model: Any,\n",
        "        query_mode: str = \"default\",\n",
        "        similarity_top_k: int = 2,\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "        self._vector_store = vector_store\n",
        "        self._embed_model = embed_model\n",
        "        self._query_mode = query_mode\n",
        "        self._similarity_top_k = similarity_top_k\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        \"\"\"Retrieve.\"\"\"\n",
        "        query_embedding = embed_model.get_query_embedding(query_str)\n",
        "        vector_store_query = VectorStoreQuery(\n",
        "            query_embedding=query_embedding,\n",
        "            similarity_top_k=self._similarity_top_k,\n",
        "            mode=self._query_mode,\n",
        "        )\n",
        "        query_result = vector_store.query(vector_store_query)\n",
        "\n",
        "        nodes_with_scores = []\n",
        "        for index, node in enumerate(query_result.nodes):\n",
        "            score: Optional[float] = None\n",
        "            if query_result.similarities is not None:\n",
        "                score = query_result.similarities[index]\n",
        "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
        "\n",
        "        return nodes_with_scores"
      ],
      "metadata": {
        "id": "IQ2DPerBjvr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = PineconeRetriever(\n",
        "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
        ")"
      ],
      "metadata": {
        "id": "9WmWsoNgkGBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_nodes = retriever.retrieve(query_str)\n",
        "for node in retrieved_nodes:\n",
        "    display_source_node(node, source_length=1000)"
      ],
      "metadata": {
        "id": "H04rMBCpkHyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plug this into our RetrieverQueryEngine to synthesize a response\n",
        "\n",
        "NOTE: We’ll cover more on how to build response synthesis from scratch in future tutorials!"
      ],
      "metadata": {
        "id": "5hOD3kQNkMkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(retriever)"
      ],
      "metadata": {
        "id": "vM7yJBkgkJV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(query_str)"
      ],
      "metadata": {
        "id": "ZtIca0VdkQ0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "id": "FSOE9bnUkR51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Response Synthesis from Scratch\n",
        "\n",
        "We’ll walk through some synthesis strategies:\n",
        "\n",
        "Create and Refine\n",
        "\n",
        "Tree Summarization\n",
        "\n",
        "We’re essentially unpacking our “Response Synthesis” module and exposing that for the user.\n",
        "\n",
        "We use OpenAI as a default LLM but you’re free to plug in any LLM you wish."
      ],
      "metadata": {
        "id": "thcURsNIlu7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup Pinecone and Load data\n",
        "\n",
        "We build an empty Pinecone Index, and define the necessary LlamaIndex wrappers/abstractions so that we can load/index data and get back a vector retriever.\n",
        "\n"
      ],
      "metadata": {
        "id": "EsFrUCB0ly4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "\n",
        "pinecone.init(api_key=\"\", environment=\"us-west1-gcp-free\")\n",
        "\n",
        "index_name = \"\"\n",
        "\n",
        "pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "\n",
        "from pathlib import Path\n",
        "from llama_hub.file.pymu_pdf.base import PyMuPDFReader\n",
        "\n",
        "file_path = \"...\"\n",
        "loader = PyMuPDFReader()\n",
        "documents = loader.load(file_path=file_path)"
      ],
      "metadata": {
        "id": "xnGEkMBWmAem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get vector retriever"
      ],
      "metadata": {
        "id": "sC8tQWOnmek5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.storage import StorageContext"
      ],
      "metadata": {
        "id": "0ouoGQ4XmV14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "# NOTE: set chunk size of 1024\n",
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context, storage_context=storage_context\n",
        ")"
      ],
      "metadata": {
        "id": "QihOvWc-mim5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.as_retriever()"
      ],
      "metadata": {
        "id": "_O2mlsUdmj6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Given an example question, get a retrieved set of nodes.\n",
        "\n",
        "We use the retriever to get a set of relevant nodes given a user query. These nodes will then be passed to the response synthesis modules below."
      ],
      "metadata": {
        "id": "PxIB4riamolQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = (\n",
        "    \"Poderia me falar quais argumentos são os mais relevantes a serem rebatidos\"\n",
        "    \" na peça do autor?\"\n",
        ")"
      ],
      "metadata": {
        "id": "fHtjP4cdml_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_nodes = retriever.retrieve(query_str)"
      ],
      "metadata": {
        "id": "i_-83xbMm4A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Response Synthesis with LLMs\n",
        "\n",
        "In this section we’ll show how to use LLMs + Prompts to build a response synthesis module.\n",
        "\n",
        "We’ll start from simple strategies (simply stuffing context into a prompt), to more advanced strategies that can handle context overflows."
      ],
      "metadata": {
        "id": "kOBXyV1xm6LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Try a Simple Prompt\n",
        "\n",
        "We first try to synthesize the response using a single input prompt + LLM call.\n",
        "\n"
      ],
      "metadata": {
        "id": "v_W6QD7im-tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "from llama_index.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-003\")"
      ],
      "metadata": {
        "id": "RbcJa68Wm5Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt = PromptTemplate(\n",
        "    \"\"\"\\\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer: \\\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "o-WTDd2BnBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an example question, retrieve the set of relevant nodes and try to put it all in the prompt, separated by newlines.\n",
        "\n"
      ],
      "metadata": {
        "id": "aabbJXtrnHAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = (\n",
        "    \"Poderia me falar quais argumentos são os mais relevantes a serem rebatidos\"\n",
        "    \" na peça do autor?\"\n",
        ")"
      ],
      "metadata": {
        "id": "j35buDZ6nEF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_nodes = retriever.retrieve(query_str)"
      ],
      "metadata": {
        "id": "E0jGkkuOnFkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(retrieved_nodes, query_str, qa_prompt, llm):\n",
        "    context_str = \"\\n\\n\".join([r.get_content() for r in retrieved_nodes])\n",
        "    fmt_qa_prompt = qa_prompt.format(\n",
        "        context_str=context_str, query_str=query_str\n",
        "    )\n",
        "    response = llm.complete(fmt_qa_prompt)\n",
        "    return str(response), fmt_qa_prompt"
      ],
      "metadata": {
        "id": "MmomS_U2nK2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response, fmt_qa_prompt = generate_response(\n",
        "    retrieved_nodes, query_str, qa_prompt, llm\n",
        ")"
      ],
      "metadata": {
        "id": "5obSKwKknMm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"*****Response******:\\n{response}\\n\\n\")"
      ],
      "metadata": {
        "id": "B6oIyw4PnN1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"*****Formatted Prompt*****:\\n{fmt_qa_prompt}\\n\\n\")\n"
      ],
      "metadata": {
        "id": "EbRMvf8rnQAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem: What if we set the top-k retriever to a higher value? The context would overflow!"
      ],
      "metadata": {
        "id": "rZQuyvlbna_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.as_retriever(similarity_top_k=6)\n",
        "retrieved_nodes = retriever.retrieve(query_str)"
      ],
      "metadata": {
        "id": "R8fXLGqYnUI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response, fmt_qa_prompt = generate_response(\n",
        "    retrieved_nodes, query_str, qa_prompt, llm\n",
        ")\n",
        "print(f\"Response (k=5): {response}\")"
      ],
      "metadata": {
        "id": "tw6Xewk-nct3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Try a “Create and Refine” strategy\n",
        "\n",
        "To deal with context overflows, we can try a strategy where we synthesize a response sequentially through all nodes. Start with the first node and generate an initial response. Then for subsequent nodes, refine the answer using additional context.\n",
        "\n",
        "This requires us to define a “refine” prompt as well."
      ],
      "metadata": {
        "id": "ihzDLEuQntj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "refine_prompt = PromptTemplate(\n",
        "    \"\"\"\\\n",
        "The original query is as follows: {query_str}\n",
        "We have provided an existing answer: {existing_answer}\n",
        "We have the opportunity to refine the existing answer \\\n",
        "(only if needed) with some more context below.\n",
        "------------\n",
        "{context_str}\n",
        "------------\n",
        "Given the new context, refine the original answer to better answer the query. \\\n",
        "If the context isn't useful, return the original answer.\n",
        "Refined Answer: \\\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "M-T2A52YneRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.response.notebook_utils import display_source_node\n",
        "\n",
        "\n",
        "def generate_response_cr(\n",
        "    retrieved_nodes, query_str, qa_prompt, refine_prompt, llm\n",
        "):\n",
        "    \"\"\"Generate a response using create and refine strategy.\n",
        "\n",
        "    The first node uses the 'QA' prompt.\n",
        "    All subsequent nodes use the 'refine' prompt.\n",
        "\n",
        "    \"\"\"\n",
        "    cur_response = None\n",
        "    fmt_prompts = []\n",
        "    for idx, node in enumerate(retrieved_nodes):\n",
        "        print(f\"[Node {idx}]\")\n",
        "        display_source_node(node, source_length=2000)\n",
        "        context_str = node.get_content()\n",
        "        if idx == 0:\n",
        "            fmt_prompt = qa_prompt.format(\n",
        "                context_str=context_str, query_str=query_str\n",
        "            )\n",
        "        else:\n",
        "            fmt_prompt = refine_prompt.format(\n",
        "                context_str=context_str,\n",
        "                query_str=query_str,\n",
        "                existing_answer=str(cur_response),\n",
        "            )\n",
        "\n",
        "        cur_response = llm.complete(fmt_prompt)\n",
        "        fmt_prompts.append(fmt_prompt)\n",
        "\n",
        "    return str(cur_response), fmt_prompts"
      ],
      "metadata": {
        "id": "a6CyZLCenzFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response, fmt_prompts = generate_response_cr(\n",
        "    retrieved_nodes, query_str, qa_prompt, refine_prompt, llm\n",
        ")"
      ],
      "metadata": {
        "id": "VjDVofYwn1X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))\n"
      ],
      "metadata": {
        "id": "Mkrx69GyoJ9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view a sample qa prompt\n",
        "print(fmt_prompts[0])"
      ],
      "metadata": {
        "id": "s2NSOZ3eoL-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view a sample refine prompt\n",
        "print(fmt_prompts[1])"
      ],
      "metadata": {
        "id": "wHB7Us00odUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: This is an initial step, but obviously there are inefficiencies. One is the fact that it’s quite slow - we make sequential calls. The second piece is that each LLM call is inefficient - we are only inserting a single node, but not “stuffing” the prompt with as much context as necessary."
      ],
      "metadata": {
        "id": "SeQbCpKnolkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Try a Hierarchical Summarization Strategy\n",
        "\n",
        "Another approach is to try a hierarchical summarization strategy. We generate an answer for each node independently, and then hierarchically combine the answers. This “combine” step could happen once, or for maximum generality can happen recursively until there is one “root” node. That “root” node is then returned as the answer.\n",
        "\n",
        "We implement this approach below. We have a fixed number of children of 5, so we hierarchically combine 5 children at a time.\n",
        "\n",
        "NOTE: In LlamaIndex this is referred to as “tree_summarize”, in LangChain this is referred to as map-reduce."
      ],
      "metadata": {
        "id": "-X5LrL_Mon9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_results(\n",
        "    texts,\n",
        "    query_str,\n",
        "    qa_prompt,\n",
        "    llm,\n",
        "    cur_prompt_list,\n",
        "    num_children=10,\n",
        "):\n",
        "    new_texts = []\n",
        "    for idx in range(0, len(texts), num_children):\n",
        "        text_batch = texts[idx : idx + num_children]\n",
        "        context_str = \"\\n\\n\".join([t for t in text_batch])\n",
        "        fmt_qa_prompt = qa_prompt.format(\n",
        "            context_str=context_str, query_str=query_str\n",
        "        )\n",
        "        combined_response = llm.complete(fmt_qa_prompt)\n",
        "        new_texts.append(str(combined_response))\n",
        "        cur_prompt_list.append(fmt_qa_prompt)\n",
        "\n",
        "    if len(new_texts) == 1:\n",
        "        return new_texts[0]\n",
        "    else:\n",
        "        return combine_results(\n",
        "            new_texts, query_str, qa_prompt, llm, num_children=num_children\n",
        "        )\n",
        "\n",
        "\n",
        "def generate_response_hs(\n",
        "    retrieved_nodes, query_str, qa_prompt, llm, num_children=10\n",
        "):\n",
        "    \"\"\"Generate a response using hierarchical summarization strategy.\n",
        "\n",
        "    Combine num_children nodes hierarchically until we get one root node.\n",
        "\n",
        "    \"\"\"\n",
        "    fmt_prompts = []\n",
        "    node_responses = []\n",
        "    for node in retrieved_nodes:\n",
        "        context_str = node.get_content()\n",
        "        fmt_qa_prompt = qa_prompt.format(\n",
        "            context_str=context_str, query_str=query_str\n",
        "        )\n",
        "        node_response = llm.complete(fmt_qa_prompt)\n",
        "        node_responses.append(node_response)\n",
        "        fmt_prompts.append(fmt_qa_prompt)\n",
        "\n",
        "    response_txt = combine_results(\n",
        "        [str(r) for r in node_responses],\n",
        "        query_str,\n",
        "        qa_prompt,\n",
        "        llm,\n",
        "        fmt_prompts,\n",
        "        num_children=num_children,\n",
        "    )\n",
        "\n",
        "    return response_txt, fmt_prompts"
      ],
      "metadata": {
        "id": "9SByHwH8of8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response, fmt_prompts = generate_response_hs(\n",
        "    retrieved_nodes, query_str, qa_prompt, llm\n",
        ")"
      ],
      "metadata": {
        "id": "j8B7CFXRotgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "id": "zs24XQnXovg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: Note that the answer is much more concise than the create-and-refine approach. This is a well-known phemonenon - the reason is because hierarchical summarization tends to compress information at each stage, whereas create and refine encourages adding on more information with each node.\n",
        "\n",
        "Observation: Similar to the above section, there are inefficiencies. We are still generating an answer for each node independently that we can try to optimize away.\n",
        "\n",
        "Our ResponseSynthesizer module handles this!"
      ],
      "metadata": {
        "id": "dSGunikSpGd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. [Optional] Let’s create an async version of hierarchical summarization!\n",
        "\n",
        "A pro of the hierarchical summarization approach is that the LLM calls can be parallelized, leading to big speedups in response synthesis.\n",
        "\n",
        "We implement an async version below. We use asyncio.gather to execute coroutines (LLM calls) for each Node concurrently."
      ],
      "metadata": {
        "id": "gTHOQlJvpIZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "Ylq6WkGoo82y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def acombine_results(\n",
        "    texts,\n",
        "    query_str,\n",
        "    qa_prompt,\n",
        "    llm,\n",
        "    cur_prompt_list,\n",
        "    num_children=10,\n",
        "):\n",
        "    fmt_prompts = []\n",
        "    for idx in range(0, len(texts), num_children):\n",
        "        text_batch = texts[idx : idx + num_children]\n",
        "        context_str = \"\\n\\n\".join([t for t in text_batch])\n",
        "        fmt_qa_prompt = qa_prompt.format(\n",
        "            context_str=context_str, query_str=query_str\n",
        "        )\n",
        "        fmt_prompts.append(fmt_qa_prompt)\n",
        "        cur_prompt_list.append(fmt_qa_prompt)\n",
        "\n",
        "    tasks = [llm.acomplete(p) for p in fmt_prompts]\n",
        "    combined_responses = await asyncio.gather(*tasks)\n",
        "    new_texts = [str(r) for r in combined_responses]\n",
        "\n",
        "    if len(new_texts) == 1:\n",
        "        return new_texts[0]\n",
        "    else:\n",
        "        return await acombine_results(\n",
        "            new_texts, query_str, qa_prompt, llm, num_children=num_children\n",
        "        )\n",
        "\n",
        "\n",
        "async def agenerate_response_hs(\n",
        "    retrieved_nodes, query_str, qa_prompt, llm, num_children=10\n",
        "):\n",
        "    \"\"\"Generate a response using hierarchical summarization strategy.\n",
        "\n",
        "    Combine num_children nodes hierarchically until we get one root node.\n",
        "\n",
        "    \"\"\"\n",
        "    fmt_prompts = []\n",
        "    node_responses = []\n",
        "    for node in retrieved_nodes:\n",
        "        context_str = node.get_content()\n",
        "        fmt_qa_prompt = qa_prompt.format(\n",
        "            context_str=context_str, query_str=query_str\n",
        "        )\n",
        "        fmt_prompts.append(fmt_qa_prompt)\n",
        "\n",
        "    tasks = [llm.acomplete(p) for p in fmt_prompts]\n",
        "    node_responses = await asyncio.gather(*tasks)\n",
        "\n",
        "    response_txt = combine_results(\n",
        "        [str(r) for r in node_responses],\n",
        "        query_str,\n",
        "        qa_prompt,\n",
        "        llm,\n",
        "        fmt_prompts,\n",
        "        num_children=num_children,\n",
        "    )\n",
        "\n",
        "    return response_txt, fmt_prompts"
      ],
      "metadata": {
        "id": "HT1F_CVJpMMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response, fmt_prompts = await agenerate_response_hs(\n",
        "    retrieved_nodes, query_str, qa_prompt, llm\n",
        ")"
      ],
      "metadata": {
        "id": "yk_uM10QpObe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "id": "_OOBSu6apPnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let’s put it all together!\n",
        "\n",
        "Let’s define a simple query engine that can be initialized with a retriever, prompt, llm etc. And have it implement a simple query function. We also implement an async version, can be used if you completed part 4 above!\n",
        "\n",
        "NOTE: We skip subclassing our own QueryEngine abstractions. This is a big TODO to make it more easily sub-classable!"
      ],
      "metadata": {
        "id": "5qNz8KrbpV0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers import BaseRetriever\n",
        "from llama_index.llms.base import LLM\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Response:\n",
        "    response: str\n",
        "    source_nodes: Optional[List] = None\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.response\n",
        "\n",
        "\n",
        "class MyQueryEngine:\n",
        "    \"\"\"My query engine.\n",
        "\n",
        "    Uses the tree summarize response synthesis module by default.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        retriever: BaseRetriever,\n",
        "        qa_prompt: PromptTemplate,\n",
        "        llm: LLM,\n",
        "        num_children=10,\n",
        "    ) -> None:\n",
        "        self._retriever = retriever\n",
        "        self._qa_prompt = qa_prompt\n",
        "        self._llm = llm\n",
        "        self._num_children = num_children\n",
        "\n",
        "    def query(self, query_str: str):\n",
        "        retrieved_nodes = self._retriever.retrieve(query_str)\n",
        "        response_txt, _ = generate_response_hs(\n",
        "            retrieved_nodes,\n",
        "            query_str,\n",
        "            self._qa_prompt,\n",
        "            self._llm,\n",
        "            num_children=self._num_children,\n",
        "        )\n",
        "        response = Response(response_txt, source_nodes=retrieved_nodes)\n",
        "        return response\n",
        "\n",
        "    async def aquery(self, query_str: str):\n",
        "        retrieved_nodes = await self._retriever.aretrieve(query_str)\n",
        "        response_txt, _ = await agenerate_response_hs(\n",
        "            retrieved_nodes,\n",
        "            query_str,\n",
        "            self._qa_prompt,\n",
        "            self._llm,\n",
        "            num_children=self._num_children,\n",
        "        )\n",
        "        response = Response(response_txt, source_nodes=retrieved_nodes)\n",
        "        return response"
      ],
      "metadata": {
        "id": "SKZUuv5jpSMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = MyQueryEngine(retriever, qa_prompt, llm, num_children=10)"
      ],
      "metadata": {
        "id": "Qzmfa548paoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(query_str)"
      ],
      "metadata": {
        "id": "NucW_9CspcmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "id": "bcNIgCwrpdsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = await query_engine.aquery(query_str)"
      ],
      "metadata": {
        "id": "peJLmo8zpk3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(response))"
      ],
      "metadata": {
        "id": "al9ntQbFpn4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Evaluation from Scratch\n",
        "\n",
        "We show how you can build evaluation modules from scratch. This includes both evaluation of the final generated response (where the output is plain text), as well as the evaluation of retrievers (where the output is a ranked list of items).\n",
        "\n",
        "We have in-house modules in our Evaluation section.\n",
        "\n",
        "##Setup Pinecone and Load data\n",
        "We load some data and define a very simple RAG query engine that we’ll evaluate (uses top-k retrieval)."
      ],
      "metadata": {
        "id": "b9KUxm6iOw8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "\n",
        "pinecone.init(api_key=\"\", environment=\"us-west1-gcp-free\")\n",
        "\n",
        "index_name = \"\"\n",
        "\n",
        "pinecone_index = pinecone.Index(index_name)\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
        "\n",
        "from pathlib import Path\n",
        "from llama_hub.file.pymu_pdf.base import PyMuPDFReader\n",
        "\n",
        "file_path = \"\"\n",
        "loader = PyMuPDFReader()\n",
        "documents = loader.load(file_path=file_path)"
      ],
      "metadata": {
        "id": "CDOcH743pr3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.llms import OpenAI"
      ],
      "metadata": {
        "id": "3Xad_12CTpLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = \"\"\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\")\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
        "service_context = ServiceContext.from_defaults(llm=llm)"
      ],
      "metadata": {
        "id": "iaSR1UmeZX5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = node_parser.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "MQRDKzOCaLrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex(nodes, service_context=service_context)"
      ],
      "metadata": {
        "id": "tsPjLFNGaaxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "3in6X1BVacCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Generation\n",
        "We first go through an exercise of generating a synthetic evaluation dataset. We do this by synthetically generating a set of questions from existing context. We then run each question with existing context through a powerful LLM (e.g. GPT-4) to generate a “ground-truth” response.\n",
        "\n",
        "##Define Functions\n",
        "We define the functions that we will use for dataset generation:"
      ],
      "metadata": {
        "id": "QgHuLGxIagDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.schema import BaseNode\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.prompts import (\n",
        "    ChatMessage,\n",
        "    ChatPromptTemplate,\n",
        "    MessageRole,\n",
        "    PromptTemplate,\n",
        ")\n",
        "from typing import Tuple, List\n",
        "import re\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "jBuzHcrGad8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define generate_answers_for_questions to generate answers from questions given context."
      ],
      "metadata": {
        "id": "rd3eLV9meWr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QA_PROMPT = PromptTemplate(\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the query.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "\n",
        "def generate_answers_for_questions(\n",
        "    questions: List[str], context: str, llm: OpenAI\n",
        ") -> str:\n",
        "    \"\"\"Generate answers for questions given context.\"\"\"\n",
        "    answers = []\n",
        "    for question in questions:\n",
        "        fmt_qa_prompt = QA_PROMPT.format(\n",
        "            context_str=context, query_str=question\n",
        "        )\n",
        "        response_obj = llm.complete(fmt_qa_prompt)\n",
        "        answers.append(str(response_obj))\n",
        "    return answers"
      ],
      "metadata": {
        "id": "ifTOShQreVPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define generate_qa_pairs to generate qa pairs over an entire list of Nodes.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dqlw5aGFeav0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION_GEN_USER_TMPL = (\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"generate the relevant questions. \"\n",
        ")\n",
        "\n",
        "QUESTION_GEN_SYS_TMPL = \"\"\"\\\n",
        "You are a Teacher/ Professor. Your task is to setup \\\n",
        "{num_questions_per_chunk} questions for an upcoming \\\n",
        "quiz/examination. The questions should be diverse in nature \\\n",
        "across the document. Restrict the questions to the \\\n",
        "context information provided.\\\n",
        "\"\"\"\n",
        "\n",
        "question_gen_template = ChatPromptTemplate(\n",
        "    message_templates=[\n",
        "        ChatMessage(role=MessageRole.SYSTEM, content=QUESTION_GEN_SYS_TMPL),\n",
        "        ChatMessage(role=MessageRole.USER, content=QUESTION_GEN_USER_TMPL),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def generate_qa_pairs(\n",
        "    nodes: List[BaseNode], llm: OpenAI, num_questions_per_chunk: int = 10\n",
        ") -> List[Tuple[str, str]]:\n",
        "    \"\"\"Generate questions.\"\"\"\n",
        "    qa_pairs = []\n",
        "    for idx, node in enumerate(nodes):\n",
        "        print(f\"Node {idx}/{len(nodes)}\")\n",
        "        context_str = node.get_content(metadata_mode=\"all\")\n",
        "        fmt_messages = question_gen_template.format_messages(\n",
        "            num_questions_per_chunk=10,\n",
        "            context_str=context_str,\n",
        "        )\n",
        "        chat_response = llm.chat(fmt_messages)\n",
        "        raw_output = chat_response.message.content\n",
        "        result_list = str(raw_output).strip().split(\"\\n\")\n",
        "        cleaned_questions = [\n",
        "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip()\n",
        "            for question in result_list\n",
        "        ]\n",
        "        answers = generate_answers_for_questions(\n",
        "            cleaned_questions, context_str, llm\n",
        "        )\n",
        "        cur_qa_pairs = list(zip(cleaned_questions, answers))\n",
        "        qa_pairs.extend(cur_qa_pairs)\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "t-PS8cJJeZPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pairs"
      ],
      "metadata": {
        "id": "I91xB_RFehVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Pairs over Dataset\n",
        "\n",
        "NOTE: This can take a long time. For the sake of speed try inputting a subset of the nodes."
      ],
      "metadata": {
        "id": "fUCS6LiVfULQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pairs = generate_qa_pairs(\n",
        "    nodes[:1],\n",
        "    llm,\n",
        "    num_questions_per_chunk=10,\n",
        ")"
      ],
      "metadata": {
        "id": "SQ3opwDpejAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Define save/load"
      ],
      "metadata": {
        "id": "k3kSci5ClAII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(qa_pairs, open(\"eval_dataset.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "LxT9GdLCfWrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "qa_pairs = pickle.load(open(\"eval_dataset.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "TLmAce96lFvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating Generation\n",
        "In this section we walk through a few methods for evaluating the generated results. At a high-level we use an “evaluation LLM” to measure the quality of the generated results. We do this in both the with labels setting and without labels setting.\n",
        "\n",
        "We go through the following evaluation algorithms:\n",
        "\n",
        "Correctness: Compares the generated answer against the ground-truth answer.\n",
        "\n",
        "Faithfulness: Evaluates whether a response is faithful to the contexts (label-free).\n",
        "\n",
        "##Building a Correctness Evaluator\n",
        "The correctness evaluator compares the generated answer to the reference ground-truth answer, given the query. We output a score between 1 and 5, where 1 is the worst and 5 is the best.\n",
        "\n",
        "We do this through a system and user prompt with a chat interface."
      ],
      "metadata": {
        "id": "QQ5VCP07lJrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.prompts import (\n",
        "    ChatMessage,\n",
        "    ChatPromptTemplate,\n",
        "    MessageRole,\n",
        "    PromptTemplate,\n",
        ")\n",
        "from typing import Dict"
      ],
      "metadata": {
        "id": "qTa2LsKTlMpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CORRECTNESS_SYS_TMPL = \"\"\"\n",
        "You are an expert evaluation system for a question answering chatbot.\n",
        "\n",
        "You are given the following information:\n",
        "- a user query,\n",
        "- a reference answer, and\n",
        "- a generated answer.\n",
        "\n",
        "Your job is to judge the relevance and correctness of the generated answer.\n",
        "Output a single score that represents a holistic evaluation.\n",
        "You must return your response in a line with only the score.\n",
        "Do not return answers in any other format.\n",
        "On a separate line provide your reasoning for the score as well.\n",
        "\n",
        "Follow these guidelines for scoring:\n",
        "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
        "- If the generated answer is not relevant to the user query, \\\n",
        "you should give a score of 1.\n",
        "- If the generated answer is relevant but contains mistakes, \\\n",
        "you should give a score between 2 and 3.\n",
        "- If the generated answer is relevant and fully correct, \\\n",
        "you should give a score between 4 and 5.\n",
        "\"\"\"\n",
        "\n",
        "CORRECTNESS_USER_TMPL = \"\"\"\n",
        "## User Query\n",
        "{query}\n",
        "\n",
        "## Reference Answer\n",
        "{reference_answer}\n",
        "\n",
        "## Generated Answer\n",
        "{generated_answer}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O80DX_3UlPCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_chat_template = ChatPromptTemplate(\n",
        "    message_templates=[\n",
        "        ChatMessage(role=MessageRole.SYSTEM, content=CORRECTNESS_SYS_TMPL),\n",
        "        ChatMessage(role=MessageRole.USER, content=CORRECTNESS_USER_TMPL),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "GI7tbYZClR5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we’ve defined the prompts template, let’s define an evaluation function that feeds the prompt to the LLM and parses the output into a dict of results.\n",
        "\n"
      ],
      "metadata": {
        "id": "2SB-o5KXlUOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "\n",
        "\n",
        "def run_correctness_eval(\n",
        "    query_str: str,\n",
        "    reference_answer: str,\n",
        "    generated_answer: str,\n",
        "    llm: OpenAI,\n",
        "    threshold: float = 4.0,\n",
        ") -> Dict:\n",
        "    \"\"\"Run correctness eval.\"\"\"\n",
        "    fmt_messages = eval_chat_template.format_messages(\n",
        "        llm=llm,\n",
        "        query=query_str,\n",
        "        reference_answer=reference_answer,\n",
        "        generated_answer=generated_answer,\n",
        "    )\n",
        "    chat_response = llm.chat(fmt_messages)\n",
        "    raw_output = chat_response.message.content\n",
        "\n",
        "    # Extract from response\n",
        "    score_str, reasoning_str = raw_output.split(\"\\n\", 1)\n",
        "    score = float(score_str)\n",
        "    reasoning = reasoning_str.lstrip(\"\\n\")\n",
        "\n",
        "    return {\"passing\": score >= threshold, \"score\": score, \"reason\": reasoning}"
      ],
      "metadata": {
        "id": "SPNb8_wxlTKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s try running this on some sample inputs with a chat model (GPT-4).\n",
        "\n"
      ],
      "metadata": {
        "id": "a57u2BnNlYPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-4\")\n"
      ],
      "metadata": {
        "id": "8zL41nsllXSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_str = (\n",
        "    \"Qual é a materia da peça processual? \"\n",
        ")\n",
        "reference_answer = (\n",
        "    \"Pedido de indenização por negativação indevida nos orgaos de proteção ao credito\"\n",
        ")"
      ],
      "metadata": {
        "id": "K9fu7vKllZox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_answer = str(query_engine.query(query_str))\n"
      ],
      "metadata": {
        "id": "ZD5P9spJldeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(str(generated_answer))\n"
      ],
      "metadata": {
        "id": "2W06IKGHlefS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = run_correctness_eval(\n",
        "    query_str, reference_answer, generated_answer, llm=llm, threshold=4.0\n",
        ")\n",
        "display(eval_results)"
      ],
      "metadata": {
        "id": "9xtKd4RklgFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a Faithfulness Evaluator\n",
        "The faithfulness evaluator evaluates whether the response is faithful to any of the retrieved contexts.\n",
        "\n",
        "This is a step up in complexity from the correctness evaluator. Since the set of contexts can be quite long, they might overflow the context window. We would need to figure out how to implement a form of response synthesis strategy to iterate over contexts in sequence.\n",
        "\n",
        "We have a corresponding tutorial showing you how to build response synthesis from scratch. We also have out-of-the-box response synthesis modules. In this guide we’ll use the out of the box modules."
      ],
      "metadata": {
        "id": "AU6Hvlu5liax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_TEMPLATE = PromptTemplate(\n",
        "    \"Please tell if a given piece of information \"\n",
        "    \"is supported by the context.\\n\"\n",
        "    \"You need to answer with either YES or NO.\\n\"\n",
        "    \"Answer YES if any of the context supports the information, even \"\n",
        "    \"if most of the context is unrelated. \"\n",
        "    \"Some examples are provided below. \\n\\n\"\n",
        "    \"Information: Apple pie is generally double-crusted.\\n\"\n",
        "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
        "    \"ingredient is apples. \\n\"\n",
        "    \"Apple pie is often served with whipped cream, ice cream \"\n",
        "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
        "    \"It is generally double-crusted, with pastry both above \"\n",
        "    \"and below the filling; the upper crust may be solid or \"\n",
        "    \"latticed (woven of crosswise strips).\\n\"\n",
        "    \"Answer: YES\\n\"\n",
        "    \"Information: Apple pies tastes bad.\\n\"\n",
        "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
        "    \"ingredient is apples. \\n\"\n",
        "    \"Apple pie is often served with whipped cream, ice cream \"\n",
        "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
        "    \"It is generally double-crusted, with pastry both above \"\n",
        "    \"and below the filling; the upper crust may be solid or \"\n",
        "    \"latticed (woven of crosswise strips).\\n\"\n",
        "    \"Answer: NO\\n\"\n",
        "    \"Information: {query_str}\\n\"\n",
        "    \"Context: {context_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "EVAL_REFINE_TEMPLATE = PromptTemplate(\n",
        "    \"We want to understand if the following information is present \"\n",
        "    \"in the context information: {query_str}\\n\"\n",
        "    \"We have provided an existing YES/NO answer: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing answer \"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_msg}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"If the existing answer was already YES, still answer YES. \"\n",
        "    \"If the information is present in the new context, answer YES. \"\n",
        "    \"Otherwise answer NO.\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "xbHd88z2lkBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: In the current response synthesizer setup we don’t separate out a system and user message for chat endpoints, so we just use our standard llm.complete for text completion.\n",
        "\n",
        "We now define our function below. Since we defined both a standard eval template for a given piece of context but also a refine template for subsequent contexts, we implement our “create-and-refine” response synthesis strategy to obtain the answer."
      ],
      "metadata": {
        "id": "PHPZWR39loKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.response_synthesizers import Refine\n",
        "from llama_index import ServiceContext\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "def run_faithfulness_eval(\n",
        "    generated_answer: str,\n",
        "    contexts: List[str],\n",
        "    llm: OpenAI,\n",
        ") -> Dict:\n",
        "    \"\"\"Run faithfulness eval.\"\"\"\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(llm=llm)\n",
        "    refine = Refine(\n",
        "        text_qa_template=EVAL_TEMPLATE,\n",
        "        refine_template=EVAL_REFINE_TEMPLATE,\n",
        "    )\n",
        "\n",
        "    response_obj = refine.get_response(generated_answer, contexts)\n",
        "    response_txt = str(response_obj)\n",
        "\n",
        "    if \"yes\" in response_txt.lower():\n",
        "        passing = True\n",
        "    else:\n",
        "        passing = False\n",
        "\n",
        "    return {\"passing\": passing, \"reason\": str(response_txt)}"
      ],
      "metadata": {
        "id": "bVEMqmuTlnKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try it out on some data\n",
        "\n"
      ],
      "metadata": {
        "id": "U50a2ylClrI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the same query_str, and reference_answer as above\n",
        "# query_str = \"What is the specific name given to the fine-tuned LLMs optimized for dialogue use cases?\"\n",
        "# reference_answer = \"The specific name given to the fine-tuned LLMs optimized for dialogue use cases is Llama 2-Chat.\"\n",
        "\n",
        "response = query_engine.query(query_str)\n",
        "generated_answer = str(response)"
      ],
      "metadata": {
        "id": "4_s1KTw5lqP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_list = [n.get_content() for n in response.source_nodes]\n",
        "eval_results = run_faithfulness_eval(\n",
        "    generated_answer,\n",
        "    contexts=context_list,\n",
        "    llm=llm,\n",
        ")\n",
        "display(eval_results)"
      ],
      "metadata": {
        "id": "IJWkw0Tqlt3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running Evaluation over our Eval Dataset\n",
        "Now let’s tie the two above sections together and run our eval modules over our eval dataset!\n",
        "\n",
        "NOTE: For the sake of speed/cost we extract a very limited sample."
      ],
      "metadata": {
        "id": "ehvw0KiWlwNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sample_size = 5\n",
        "qa_pairs_sample = random.sample(qa_pairs, sample_size)"
      ],
      "metadata": {
        "id": "yShCekxflxt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def run_evals(qa_pairs: List[Tuple[str, str]], llm: OpenAI, query_engine):\n",
        "    results_list = []\n",
        "    for question, reference_answer in qa_pairs:\n",
        "        response = query_engine.query(question)\n",
        "        generated_answer = str(response)\n",
        "        correctness_results = run_correctness_eval(\n",
        "            query_str,\n",
        "            reference_answer,\n",
        "            generated_answer,\n",
        "            llm=llm,\n",
        "            threshold=4.0,\n",
        "        )\n",
        "        faithfulness_results = run_faithfulness_eval(\n",
        "            generated_answer,\n",
        "            contexts=context_list,\n",
        "            llm=llm,\n",
        "        )\n",
        "        cur_result_dict = {\n",
        "            \"correctness\": correctness_results[\"passing\"],\n",
        "            \"faithfulness\": faithfulness_results[\"passing\"],\n",
        "        }\n",
        "        results_list.append(cur_result_dict)\n",
        "    return pd.DataFrame(results_list)"
      ],
      "metadata": {
        "id": "OtcfxgmulzZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evals_df = run_evals(qa_pairs_sample, llm, query_engine)\n"
      ],
      "metadata": {
        "id": "Ztn7TII9l0hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evals_df[\"correctness\"].mean()\n"
      ],
      "metadata": {
        "id": "G4mNhrnBl1Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evals_df[\"faithfulness\"].mean()\n"
      ],
      "metadata": {
        "id": "pYowdDVRl2PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGDoxivGpIYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}